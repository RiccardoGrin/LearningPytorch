{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LSTM from tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Variable containing:\n",
      "-0.2275 -1.1062 -0.4056\n",
      "[torch.FloatTensor of size 1x3]\n",
      ", Variable containing:\n",
      " 0.1310 -0.2171 -0.7024\n",
      "[torch.FloatTensor of size 1x3]\n",
      ", Variable containing:\n",
      " 0.3482  0.9366 -0.2694\n",
      "[torch.FloatTensor of size 1x3]\n",
      ", Variable containing:\n",
      "-2.0989  0.8756  1.1502\n",
      "[torch.FloatTensor of size 1x3]\n",
      ", Variable containing:\n",
      " 0.4442 -0.7852  1.3552\n",
      "[torch.FloatTensor of size 1x3]\n",
      "]\n",
      "-------------------\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.7264 -0.6403 -0.4487\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.6554  0.7576  1.7538\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n",
      "-------------------\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0378  0.0332 -0.1123\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0378  0.0332 -0.1123\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0631  0.1178 -0.3186\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [autograd.Variable(torch.randn((1, 3))) for _ in range(5)]  # make a sequence of length 5\n",
    "print(inputs)\n",
    "print(\"-------------------\")\n",
    "# initialize the hidden state.\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn((1, 1, 3))))\n",
    "print(hidden)\n",
    "print(\"-------------------\")\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.1831  0.1643 -0.5233\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.2461 -0.0436 -0.3136\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.0793 -0.1841 -0.1556\n",
      "\n",
      "(3 ,.,.) = \n",
      "  0.1875 -0.1087 -0.2551\n",
      "\n",
      "(4 ,.,.) = \n",
      "  0.1812 -0.0302 -0.2254\n",
      "[torch.FloatTensor of size 5x1x3]\n",
      "\n",
      "---------------------------------------\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.1812 -0.0302 -0.2254\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.3162 -0.1519 -0.4503\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "inputs = [autograd.Variable(torch.randn((1, 3))) for _ in range(5)]  # make a sequence of length 5\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "# print(inputs)\n",
    "# print(\"-------------------\")\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn((1, 1, 3))))  # clean out hidden state\n",
    "# print(hidden)\n",
    "# print(\"-------------------\")\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(\"---------------------------------------\")\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      " [(['The', 'dog', 'ate', 'the', 'apple'], ['DET', 'NN', 'V', 'DET', 'NN']), (['Everybody', 'read', 'that', 'book'], ['NN', 'V', 'DET', 'NN'])]\n",
      "\n",
      "Word dict: \n",
      " {'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "print(\"Training data: \\n\", training_data)\n",
    "\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(\"\\nWord dict: \\n\", word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.4350 -1.2465 -0.7458\n",
      "-1.4579 -1.1735 -0.7809\n",
      "-1.3504 -1.1555 -0.8534\n",
      "-1.3558 -1.2744 -0.7708\n",
      "-1.3321 -1.2605 -0.7928\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "Variable containing:\n",
      "-5.4403 -0.0436 -3.2620\n",
      "-5.9751 -0.0718 -2.7075\n",
      "-6.1204 -0.0712 -2.7100\n",
      "-6.1765 -0.0672 -2.7659\n",
      "-6.2042 -0.0648 -2.8016\n",
      "-6.2193 -0.0635 -2.8212\n",
      "[torch.FloatTensor of size 6x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "#         print(\"Input: \", sentence)\n",
    "#         print(\"Tags: \", tags)\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Variables of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "#         print(\"Prep input: \", sentence_in)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "#         print(\"Prep tag: \", targets)\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "#         print(\"Scores: \", tag_scores)\n",
    "        \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "inputs = prepare_sequence(['dog', 'dog', 'dog', 'dog', 'dog', 'dog',], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#  for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dict(training_data):\n",
    "    word_to_ix = {}\n",
    "    for sent in training_data:\n",
    "        for word in sent:\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "    print(word_to_ix)\n",
    "    return word_to_ix\n",
    "    \n",
    "def create_data(sentences):\n",
    "    training_data = []\n",
    "    for s in sentences:\n",
    "        for i in range(len(s)):\n",
    "            training_data.append(s)\n",
    "            s = np.roll(s,-1).tolist()\n",
    "            \n",
    "            \n",
    "    print(training_data)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'dog', 'ate', 'the', 'apple', '.\\n'], ['dog', 'ate', 'the', 'apple', '.\\n', 'The'], ['ate', 'the', 'apple', '.\\n', 'The', 'dog'], ['the', 'apple', '.\\n', 'The', 'dog', 'ate'], ['apple', '.\\n', 'The', 'dog', 'ate', 'the'], ['.\\n', 'The', 'dog', 'ate', 'the', 'apple'], ['One', 'cat', 'drank', 'milk', '.\\n'], ['cat', 'drank', 'milk', '.\\n', 'One'], ['drank', 'milk', '.\\n', 'One', 'cat'], ['milk', '.\\n', 'One', 'cat', 'drank'], ['.\\n', 'One', 'cat', 'drank', 'milk'], ['A', 'bird', 'flew', 'up', 'high', '.\\n'], ['bird', 'flew', 'up', 'high', '.\\n', 'A'], ['flew', 'up', 'high', '.\\n', 'A', 'bird'], ['up', 'high', '.\\n', 'A', 'bird', 'flew'], ['high', '.\\n', 'A', 'bird', 'flew', 'up'], ['.\\n', 'A', 'bird', 'flew', 'up', 'high']]\n",
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, '.\\n': 5, 'One': 6, 'cat': 7, 'drank': 8, 'milk': 9, 'A': 10, 'bird': 11, 'flew': 12, 'up': 13, 'high': 14}\n",
      "{0: 'The', 1: 'dog', 2: 'ate', 3: 'the', 4: 'apple', 5: '.\\n', 6: 'One', 7: 'cat', 8: 'drank', 9: 'milk', 10: 'A', 11: 'bird', 12: 'flew', 13: 'up', 14: 'high'}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"The dog ate the apple .\\n\".split(' '), \n",
    "             \"One cat drank milk .\\n\".split(' '),\n",
    "             \"A bird flew up high .\\n\".split(' ')]\n",
    "\n",
    "training_data = create_data(sentences)\n",
    "word_to_ix = create_dict(training_data)\n",
    "\n",
    "ix_to_word = dict([(y, x) for (y, x) in enumerate(word_to_ix)])\n",
    "print(ix_to_word)\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-2.3742 -2.6899 -2.9325 -2.9993 -2.9492 -3.1347 -3.0804 -2.3906 -2.4461 -3.0026\n",
      "-2.3930 -2.7424 -2.7529 -3.0178 -2.8780 -3.1536 -2.8952 -2.5308 -2.4040 -3.0523\n",
      "-2.5096 -2.5814 -2.8979 -3.0559 -2.7384 -3.0120 -2.9874 -2.4290 -2.2567 -3.0386\n",
      "-2.4067 -2.6595 -2.8897 -2.9771 -2.8982 -3.1246 -3.0336 -2.4616 -2.4108 -3.0186\n",
      "-2.5297 -2.6388 -2.8721 -3.0923 -2.7891 -2.9657 -2.9772 -2.3855 -2.3330 -3.0466\n",
      "-2.6422 -2.5127 -2.8755 -3.1085 -2.6131 -2.8882 -2.9223 -2.4676 -2.1940 -3.0797\n",
      "\n",
      "Columns 10 to 14 \n",
      "-2.3937 -2.4740 -2.8461 -2.6758 -2.7523\n",
      "-2.4340 -2.4921 -2.9421 -2.5274 -2.8630\n",
      "-2.5029 -2.6979 -2.9429 -2.6558 -2.7544\n",
      "-2.3869 -2.5227 -2.8723 -2.6422 -2.7755\n",
      "-2.4604 -2.6279 -2.8448 -2.6950 -2.7637\n",
      "-2.5242 -2.8483 -2.9400 -2.6899 -2.7670\n",
      "[torch.FloatTensor of size 6x15]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(word_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1000):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence in training_data:\n",
    "        sentence = random.sample(sentence, len(sentence))\n",
    "        \n",
    "        tags = np.roll(sentence,-1).tolist()\n",
    "\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Variables of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "#         print(\"Prep input: \", sentence_in)\n",
    "        targets = prepare_sequence(tags, word_to_ix)\n",
    "#         print(\"Prep tag: \", targets)\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "#         print(\"Scores: \", tag_scores)\n",
    "        \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "inputs = prepare_sequence(['The', 'dog', 'ate', 'the'], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#  for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word = 'A'\n",
    "print(input_word, end=' ')\n",
    "\n",
    "for i in range(40):\n",
    "    inputs = prepare_sequence([input_word], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    input_word = ix_to_word[int(tag_scores.max(dim=1)[1])]\n",
    "    print(input_word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
